{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module import ddpg\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "# CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING: https://arxiv.org/pdf/1509.02971\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BipedalWalker-v3', hardcore=False)\n",
    "\n",
    "# Get the state and action sizes\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "\n",
    "print(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "env = gym.make('BipedalWalker-v3', hardcore=False)\n",
    "agent = ddpg.DDPGAgent(state_size, action_size,\n",
    "                       min_start=1000, replace_step=20,\n",
    "                       lr_actor=10**-4, lr_critic=10**-4,\n",
    "                       noise_dev=0.5, noise_decay=0.999)\n",
    "score_history = []\n",
    "avg_score_history = []\n",
    "\n",
    "n_episodes = 5000\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    done = False\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state[0], [1, state_size])\n",
    "    time_step = 0  # to count number of steps in an episode\n",
    "    while not done:\n",
    "        time_step += 1\n",
    "        action = agent.act(state, evaluate=False)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = next_state.reshape(1, state_size)\n",
    "        agent.store_data(state, action, reward, next_state, done)\n",
    "        agent.learn()\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        if time_step >= 3000:\n",
    "            print(\"Break due to taking too long to learn\")\n",
    "            break\n",
    "        \n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "    avg_score_history.append(avg_score)\n",
    "    print('Episode: ', i, '- Score: ', round(score, 3), '- Average score: ', round(avg_score, 3), '- Noise ', round(agent.noise_dev, 3), '- End after: ', time_step )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BipedalWalker-v3', hardcore=False, render_mode='human')\n",
    "state = env.reset()\n",
    "state = np.reshape(state[0], [1, state_size])\n",
    "done = False\n",
    "score = 0\n",
    "while not done:\n",
    "    action = agent.act(state, evaluate=True)\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    next_state = next_state.reshape(1, state_size)\n",
    "    \n",
    "    state = next_state\n",
    "    score += reward\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(avg_score_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_size=action_size, state_size=state_size)\n",
    "state = env.reset()\n",
    "state = np.reshape(state[0], [1, state_size])\n",
    "done = False\n",
    "score = 0\n",
    "for i in range(100):\n",
    "    env.render()\n",
    "    # select action\n",
    "    action = agent.select_action(state)\n",
    "    # perform the action\n",
    "    next_state, reward, done, _, _= env.step(action)\n",
    "    # insert data to the buffer\n",
    "    agent.store_data(state, action, reward, next_state, done)\n",
    "    # update the score\n",
    "    score += reward\n",
    "    # move to the next state\n",
    "    next_state = np.reshape(next_state, [1, state_size])\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch = random.sample(agent.buffer, min(agent.buffer_size, agent.batch_size))\n",
    "states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "\n",
    "# convert to tensor, we want action to be integer\n",
    "states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape() as tape1:\n",
    "    actions_next_states = agent.actor_target(tf.squeeze(next_states))\n",
    "    Q_value_next_states = tf.squeeze(agent.critic_target(tf.concat([tf.squeeze(next_states), actions], axis=1)))\n",
    "    y = rewards + agent.gamma * Q_value_next_states * (1 - dones)\n",
    "    \n",
    "    Q_value_current_states = tf.squeeze(agent.critic_main(tf.concat([tf.squeeze(states), actions], axis=1)))\n",
    "    critic_loss = tf.reduce_mean(tf.square(y - Q_value_current_states))\n",
    "    \n",
    "with tf.GradientTape() as tape2:\n",
    "    new_actions = agent.actor_main(tf.squeeze(states))\n",
    "    actor_loss = tf.squeeze(agent.critic_main(tf.concat([tf.squeeze(states), new_actions], axis=1)))\n",
    "    actor_loss = - tf.reduce_mean(actor_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads1 = tape1.gradient(critic_loss, agent.critic_main.trainable_variables)\n",
    "grads2 = tape2.gradient(actor_loss, agent.actor_main.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.actor_main(state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dqn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
