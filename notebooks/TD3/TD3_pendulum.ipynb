{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module import td3\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "\n",
    "# https://www.gymlibrary.dev/environments/classic_control/pendulum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v1', g=9.81)\n",
    "\n",
    "# Get the state and action sizes\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "\n",
    "print(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sondinhvan/miniconda3/envs/rl_env/lib/python3.9/site-packages/gym/envs/classic_control/pendulum.py:167: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"Pendulum-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/Users/sondinhvan/miniconda3/envs/rl_env/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-578.729138331696"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "state = np.reshape(state[0], [1, state_size])\n",
    "done = False\n",
    "score = 0\n",
    "for i in range(100):\n",
    "    env.render()\n",
    "    # select action\n",
    "    action = np.random.uniform(low=-2, high=2, size=(1,))\n",
    "    # perform the action\n",
    "    next_state, reward, done, _, _= env.step(action)\n",
    "    # update the score\n",
    "    score += reward\n",
    "    # move to the next state\n",
    "    next_state = np.reshape(next_state, [1, state_size])\n",
    "    state = next_state\n",
    "\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0 - Score:  -1327.316 - Average score:  -1327.316 - Memory size:  200\n",
      "Collect enough samples, training starting\n",
      "Episode:  1 - Score:  -1380.089 - Average score:  -1353.703 - Memory size:  400\n",
      "Episode:  2 - Score:  -1423.071 - Average score:  -1376.825 - Memory size:  600\n",
      "Episode:  3 - Score:  -1853.123 - Average score:  -1495.9 - Memory size:  800\n",
      "Episode:  4 - Score:  -1636.5 - Average score:  -1524.02 - Memory size:  1000\n",
      "Episode:  5 - Score:  -1425.53 - Average score:  -1507.605 - Memory size:  1200\n",
      "Episode:  6 - Score:  -1476.412 - Average score:  -1503.149 - Memory size:  1400\n",
      "Episode:  7 - Score:  -1284.598 - Average score:  -1475.83 - Memory size:  1600\n",
      "Episode:  8 - Score:  -1364.748 - Average score:  -1463.487 - Memory size:  1800\n",
      "Episode:  9 - Score:  -1256.018 - Average score:  -1442.74 - Memory size:  2000\n",
      "Episode:  10 - Score:  -928.805 - Average score:  -1396.019 - Memory size:  2200\n",
      "Episode:  11 - Score:  -894.548 - Average score:  -1354.23 - Memory size:  2400\n",
      "Episode:  12 - Score:  -832.013 - Average score:  -1314.059 - Memory size:  2600\n",
      "Episode:  13 - Score:  -648.642 - Average score:  -1266.53 - Memory size:  2800\n",
      "Episode:  14 - Score:  -411.273 - Average score:  -1209.512 - Memory size:  3000\n",
      "Episode:  15 - Score:  -395.647 - Average score:  -1158.646 - Memory size:  3200\n",
      "Episode:  16 - Score:  -132.248 - Average score:  -1098.27 - Memory size:  3400\n",
      "Episode:  17 - Score:  -273.246 - Average score:  -1052.435 - Memory size:  3600\n",
      "Episode:  18 - Score:  -379.158 - Average score:  -1016.999 - Memory size:  3800\n",
      "Episode:  19 - Score:  -250.871 - Average score:  -978.693 - Memory size:  4000\n",
      "Episode:  20 - Score:  -249.449 - Average score:  -943.967 - Memory size:  4200\n",
      "Episode:  21 - Score:  -124.236 - Average score:  -906.706 - Memory size:  4400\n",
      "Episode:  22 - Score:  -304.889 - Average score:  -880.54 - Memory size:  4600\n",
      "Episode:  23 - Score:  -1.946 - Average score:  -843.932 - Memory size:  4800\n",
      "Episode:  24 - Score:  -248.337 - Average score:  -820.109 - Memory size:  5000\n",
      "Episode:  25 - Score:  -369.507 - Average score:  -802.778 - Memory size:  5200\n",
      "Episode:  26 - Score:  -259.351 - Average score:  -782.651 - Memory size:  5400\n",
      "Episode:  27 - Score:  -348.857 - Average score:  -767.158 - Memory size:  5600\n",
      "Episode:  28 - Score:  -1.251 - Average score:  -740.748 - Memory size:  5800\n",
      "Episode:  29 - Score:  -125.082 - Average score:  -720.225 - Memory size:  6000\n",
      "Episode:  30 - Score:  -255.669 - Average score:  -705.24 - Memory size:  6200\n",
      "Episode:  31 - Score:  -237.257 - Average score:  -690.615 - Memory size:  6400\n",
      "Episode:  32 - Score:  -270.71 - Average score:  -677.891 - Memory size:  6600\n",
      "Episode:  33 - Score:  -125.079 - Average score:  -661.632 - Memory size:  6800\n",
      "Episode:  34 - Score:  -240.257 - Average score:  -649.592 - Memory size:  7000\n",
      "Episode:  35 - Score:  -238.027 - Average score:  -638.16 - Memory size:  7200\n",
      "Episode:  36 - Score:  -127.092 - Average score:  -624.347 - Memory size:  7400\n",
      "Episode:  37 - Score:  -121.553 - Average score:  -611.116 - Memory size:  7600\n",
      "Episode:  38 - Score:  -1.282 - Average score:  -595.479 - Memory size:  7800\n",
      "Episode:  39 - Score:  -317.586 - Average score:  -588.532 - Memory size:  8000\n",
      "Episode:  40 - Score:  -126.327 - Average score:  -558.507 - Memory size:  8200\n",
      "Episode:  41 - Score:  -126.505 - Average score:  -527.168 - Memory size:  8400\n",
      "Episode:  42 - Score:  -242.866 - Average score:  -497.662 - Memory size:  8600\n",
      "Episode:  43 - Score:  -125.868 - Average score:  -454.481 - Memory size:  8800\n",
      "Episode:  44 - Score:  -130.02 - Average score:  -416.819 - Memory size:  9000\n",
      "Episode:  45 - Score:  -120.928 - Average score:  -384.204 - Memory size:  9200\n",
      "Episode:  46 - Score:  -354.674 - Average score:  -356.161 - Memory size:  9400\n",
      "Episode:  47 - Score:  -0.905 - Average score:  -324.068 - Memory size:  9600\n",
      "Episode:  48 - Score:  -126.937 - Average score:  -293.123 - Memory size:  9800\n",
      "Episode:  49 - Score:  -231.208 - Average score:  -267.503 - Memory size:  10000\n",
      "Episode:  50 - Score:  -127.388 - Average score:  -247.467 - Memory size:  10200\n",
      "Episode:  51 - Score:  -0.986 - Average score:  -225.128 - Memory size:  10400\n",
      "Episode:  52 - Score:  -351.542 - Average score:  -213.116 - Memory size:  10600\n",
      "Episode:  53 - Score:  -124.708 - Average score:  -200.018 - Memory size:  10800\n",
      "Episode:  54 - Score:  -123.609 - Average score:  -192.827 - Memory size:  11000\n",
      "Episode:  55 - Score:  -353.697 - Average score:  -191.778 - Memory size:  11200\n",
      "Episode:  56 - Score:  -117.615 - Average score:  -191.412 - Memory size:  11400\n",
      "Episode:  57 - Score:  -343.238 - Average score:  -193.162 - Memory size:  11600\n",
      "Episode:  58 - Score:  -125.47 - Average score:  -186.82 - Memory size:  11800\n",
      "Episode:  59 - Score:  -126.363 - Average score:  -183.707 - Memory size:  12000\n",
      "Episode:  60 - Score:  -119.966 - Average score:  -180.47 - Memory size:  12200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m next_state \u001b[38;5;241m=\u001b[39m next_state\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, state_size)\n\u001b[1;32m     21\u001b[0m agent\u001b[38;5;241m.\u001b[39mstore_data(state, action, reward, next_state, done)\n\u001b[0;32m---> 22\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     24\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[0;32m~/Documents/GitHub/reinforcement-learning/module/td3.py:222\u001b[0m, in \u001b[0;36mTD3Agent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_target(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_eval, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_target, tau\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.005\u001b[39m)\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_target(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_eval_1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_target_1, tau\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.005\u001b[39m)\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic_eval_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic_target_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/GitHub/reinforcement-learning/module/td3.py:125\u001b[0m, in \u001b[0;36mTD3Agent.update_target\u001b[0;34m(self, network, target_network, tau)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03mCopy the weights of the network to the target model using soft update.\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03mWhen tau = 1 -> hard update, e.g., copy the exact weights.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (weight, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(network\u001b[38;5;241m.\u001b[39mweights, target_network\u001b[38;5;241m.\u001b[39mweights):\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;66;03m# update the target values\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     \u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_env/lib/python3.9/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_env/lib/python3.9/site-packages/tensorflow/python/ops/resource_variable_ops.py:1040\u001b[0m, in \u001b[0;36mBaseResourceVariable.assign\u001b[0;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;66;03m# Note: not depending on the cached value here since this can be used to\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;66;03m# initialize the variable.\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _handle_graph(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle):\n\u001b[0;32m-> 1040\u001b[0m   value_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1041\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape\u001b[38;5;241m.\u001b[39mis_compatible_with(value_tensor\u001b[38;5;241m.\u001b[39mshape):\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_env/lib/python3.9/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_env/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:696\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;66;03m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[1;32m    695\u001b[0m preferred_dtype \u001b[38;5;241m=\u001b[39m preferred_dtype \u001b[38;5;129;01mor\u001b[39;00m dtype_hint\n\u001b[0;32m--> 696\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccepted_result_types\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_env/lib/python3.9/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:203\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts `value` to a `Tensor` using registered conversion functions.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m    value.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m   dtype \u001b[38;5;241m=\u001b[39m \u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preferred_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m   preferred_dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(preferred_dtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_env/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py:793\u001b[0m, in \u001b[0;36mas_dtype\u001b[0;34m(type_value)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Ensure no collisions.\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_ANY_TO_TF) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28msum\u001b[39m(\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28mlen\u001b[39m(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m [_INTERN_TABLE, _STRING_TO_TF, _PYTHON_TO_TF, _NP_TO_TF])\n\u001b[0;32m--> 793\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtypes.as_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mas_dtype\u001b[39m(type_value):\n\u001b[1;32m    795\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `type_value` to a `tf.DType`.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \n\u001b[1;32m    797\u001b[0m \u001b[38;5;124;03m  Inputs can be existing `tf.DType` objects, a [`DataType`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;124;03m    TypeError: If `type_value` cannot be converted to a `DType`.\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m    825\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(type_value, DType):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training\n",
    "env = gym.make('Pendulum-v1')\n",
    "agent = td3.TD3Agent(state_size, action_size, batch_size=32,\n",
    "                training_start=200, update_period=1,\n",
    "                lr_critic=0.001, lr_actor=0.001,\n",
    "                min_action=-1, max_action=1)\n",
    "score_history = []\n",
    "avg_score_history = []\n",
    "n_episodes = 20000\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    done = False\n",
    "    truncated = False\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state[0], [1, state_size])\n",
    "    while not truncated and not done:\n",
    "        action = agent.act(state, use_noise=True, noise_label='Gaussian')\n",
    "        next_state, reward, done, truncated, _ = env.step(2 * action)\n",
    "        next_state = next_state.reshape(1, state_size)\n",
    "        agent.store_data(state, action, reward, next_state, done)\n",
    "        agent.learn()\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        \n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-40:])\n",
    "    avg_score_history.append(avg_score)\n",
    "    print('Episode: ', i, '- Score: ', round(score, 3), '- Average score: ', round(avg_score, 3), '- Memory size: ', len(agent.memory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1', render_mode='human')\n",
    "state = env.reset()\n",
    "state = np.reshape(state[0], [1, state_size])\n",
    "done = False\n",
    "truncated = False\n",
    "score = 0\n",
    "while not done and not truncated:\n",
    "    action = agent.act(state, use_noise=False)\n",
    "    next_state, reward, done, truncated, _ = env.step(2 * action)\n",
    "    next_state = next_state.reshape(1, state_size)\n",
    "    state = next_state\n",
    "    score += reward\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dqn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
