{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module import td3\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "\n",
    "# https://www.gymlibrary.dev/environments/classic_control/pendulum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v1', g=9.81)\n",
    "\n",
    "# Get the state and action sizes\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "\n",
    "print(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sondinhvan/miniconda3/envs/rl_env/lib/python3.9/site-packages/gym/envs/classic_control/pendulum.py:167: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"Pendulum-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/Users/sondinhvan/miniconda3/envs/rl_env/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-771.0979688833593"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "state = np.reshape(state[0], [1, state_size])\n",
    "done = False\n",
    "score = 0\n",
    "for i in range(100):\n",
    "    env.render()\n",
    "    # select action\n",
    "    action = np.random.uniform(low=-2, high=2, size=(1,))\n",
    "    # perform the action\n",
    "    next_state, reward, done, _, _= env.step(action)\n",
    "    # update the score\n",
    "    score += reward\n",
    "    # move to the next state\n",
    "    next_state = np.reshape(next_state, [1, state_size])\n",
    "    state = next_state\n",
    "\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0 - Score:  -1582.494 - Average score:  -1582.494 - Memory size:  200\n",
      "Collect enough samples, training starting\n",
      "Episode:  1 - Score:  -1390.406 - Average score:  -1486.45 - Memory size:  400\n",
      "Episode:  2 - Score:  -1475.566 - Average score:  -1482.822 - Memory size:  600\n",
      "Episode:  3 - Score:  -1691.829 - Average score:  -1535.074 - Memory size:  800\n",
      "Episode:  4 - Score:  -1703.733 - Average score:  -1568.806 - Memory size:  1000\n",
      "Episode:  5 - Score:  -1620.455 - Average score:  -1577.414 - Memory size:  1200\n",
      "Episode:  6 - Score:  -1525.087 - Average score:  -1569.938 - Memory size:  1400\n",
      "Episode:  7 - Score:  -1387.519 - Average score:  -1547.136 - Memory size:  1600\n",
      "Episode:  8 - Score:  -1531.281 - Average score:  -1545.374 - Memory size:  1800\n",
      "Episode:  9 - Score:  -1452.845 - Average score:  -1536.121 - Memory size:  2000\n",
      "Episode:  10 - Score:  -133.788 - Average score:  -1408.637 - Memory size:  2200\n",
      "Episode:  11 - Score:  -4.782 - Average score:  -1291.649 - Memory size:  2400\n",
      "Episode:  12 - Score:  -954.852 - Average score:  -1265.741 - Memory size:  2600\n",
      "Episode:  13 - Score:  -134.15 - Average score:  -1184.913 - Memory size:  2800\n",
      "Episode:  14 - Score:  -539.791 - Average score:  -1141.905 - Memory size:  3000\n",
      "Episode:  15 - Score:  -1.219 - Average score:  -1070.612 - Memory size:  3200\n",
      "Episode:  16 - Score:  -131.808 - Average score:  -1015.388 - Memory size:  3400\n",
      "Episode:  17 - Score:  -967.47 - Average score:  -1012.726 - Memory size:  3600\n",
      "Episode:  18 - Score:  -1433.389 - Average score:  -1034.866 - Memory size:  3800\n",
      "Episode:  19 - Score:  -1380.209 - Average score:  -1052.134 - Memory size:  4000\n",
      "Episode:  20 - Score:  -660.346 - Average score:  -1033.477 - Memory size:  4200\n",
      "Episode:  21 - Score:  -130.653 - Average score:  -992.44 - Memory size:  4400\n",
      "Episode:  22 - Score:  -135.234 - Average score:  -955.17 - Memory size:  4600\n",
      "Episode:  23 - Score:  -258.892 - Average score:  -926.158 - Memory size:  4800\n",
      "Episode:  24 - Score:  -260.771 - Average score:  -899.543 - Memory size:  5000\n",
      "Episode:  25 - Score:  -246.078 - Average score:  -874.409 - Memory size:  5200\n",
      "Episode:  26 - Score:  -117.526 - Average score:  -846.377 - Memory size:  5400\n",
      "Episode:  27 - Score:  -368.791 - Average score:  -829.32 - Memory size:  5600\n",
      "Episode:  28 - Score:  -2.515 - Average score:  -800.81 - Memory size:  5800\n",
      "Episode:  29 - Score:  -131.484 - Average score:  -778.499 - Memory size:  6000\n",
      "Episode:  30 - Score:  -241.289 - Average score:  -761.169 - Memory size:  6200\n",
      "Episode:  31 - Score:  -256.003 - Average score:  -745.383 - Memory size:  6400\n",
      "Episode:  32 - Score:  -1.718 - Average score:  -722.848 - Memory size:  6600\n",
      "Episode:  33 - Score:  -124.664 - Average score:  -705.254 - Memory size:  6800\n",
      "Episode:  34 - Score:  -251.462 - Average score:  -692.289 - Memory size:  7000\n",
      "Episode:  35 - Score:  -123.876 - Average score:  -676.499 - Memory size:  7200\n",
      "Episode:  36 - Score:  -124.846 - Average score:  -661.59 - Memory size:  7400\n",
      "Episode:  37 - Score:  -125.6 - Average score:  -647.485 - Memory size:  7600\n",
      "Episode:  38 - Score:  -1063.698 - Average score:  -658.157 - Memory size:  7800\n",
      "Episode:  39 - Score:  -1177.632 - Average score:  -671.144 - Memory size:  8000\n",
      "Episode:  40 - Score:  -115.298 - Average score:  -634.464 - Memory size:  8200\n",
      "Episode:  41 - Score:  -788.167 - Average score:  -619.408 - Memory size:  8400\n",
      "Episode:  42 - Score:  -257.013 - Average score:  -588.944 - Memory size:  8600\n",
      "Episode:  43 - Score:  -129.782 - Average score:  -549.893 - Memory size:  8800\n",
      "Episode:  44 - Score:  -126.594 - Average score:  -510.464 - Memory size:  9000\n",
      "Episode:  45 - Score:  -1.398 - Average score:  -469.988 - Memory size:  9200\n",
      "Episode:  46 - Score:  -127.065 - Average score:  -435.037 - Memory size:  9400\n",
      "Episode:  47 - Score:  -283.042 - Average score:  -407.426 - Memory size:  9600\n",
      "Episode:  48 - Score:  -126.817 - Average score:  -372.314 - Memory size:  9800\n",
      "Episode:  49 - Score:  -240.092 - Average score:  -341.995 - Memory size:  10000\n",
      "Episode:  50 - Score:  -251.022 - Average score:  -344.926 - Memory size:  10200\n",
      "Episode:  51 - Score:  -124.218 - Average score:  -347.912 - Memory size:  10400\n",
      "Episode:  52 - Score:  -0.43 - Average score:  -324.051 - Memory size:  10600\n",
      "Episode:  53 - Score:  -121.288 - Average score:  -323.73 - Memory size:  10800\n",
      "Episode:  54 - Score:  -245.548 - Average score:  -316.374 - Memory size:  11000\n",
      "Episode:  55 - Score:  -127.556 - Average score:  -319.532 - Memory size:  11200\n",
      "Episode:  56 - Score:  -133.634 - Average score:  -319.578 - Memory size:  11400\n",
      "Episode:  57 - Score:  -130.167 - Average score:  -298.645 - Memory size:  11600\n",
      "Episode:  58 - Score:  -125.91 - Average score:  -265.958 - Memory size:  11800\n",
      "Episode:  59 - Score:  -245.254 - Average score:  -237.584 - Memory size:  12000\n",
      "Episode:  60 - Score:  -120.14 - Average score:  -224.079 - Memory size:  12200\n",
      "Episode:  61 - Score:  -119.582 - Average score:  -223.802 - Memory size:  12400\n",
      "Episode:  62 - Score:  -126.516 - Average score:  -223.584 - Memory size:  12600\n",
      "Episode:  63 - Score:  -119.7 - Average score:  -220.105 - Memory size:  12800\n",
      "Episode:  64 - Score:  -123.461 - Average score:  -216.672 - Memory size:  13000\n",
      "Episode:  65 - Score:  -122.179 - Average score:  -213.574 - Memory size:  13200\n",
      "Episode:  66 - Score:  -236.618 - Average score:  -216.552 - Memory size:  13400\n",
      "Episode:  67 - Score:  -129.496 - Average score:  -210.569 - Memory size:  13600\n",
      "Episode:  68 - Score:  -118.288 - Average score:  -213.464 - Memory size:  13800\n",
      "Episode:  69 - Score:  -129.274 - Average score:  -213.408 - Memory size:  14000\n",
      "Episode:  70 - Score:  -3.032 - Average score:  -207.452 - Memory size:  14200\n",
      "Episode:  71 - Score:  -122.08 - Average score:  -204.104 - Memory size:  14400\n",
      "Episode:  72 - Score:  -128.551 - Average score:  -207.275 - Memory size:  14600\n",
      "Episode:  73 - Score:  -122.648 - Average score:  -207.224 - Memory size:  14800\n",
      "Episode:  74 - Score:  -123.529 - Average score:  -204.026 - Memory size:  15000\n",
      "Episode:  75 - Score:  -255.186 - Average score:  -207.309 - Memory size:  15200\n",
      "Episode:  76 - Score:  -122.406 - Average score:  -207.248 - Memory size:  15400\n",
      "Episode:  77 - Score:  -353.222 - Average score:  -212.938 - Memory size:  15600\n",
      "Episode:  78 - Score:  -241.136 - Average score:  -192.374 - Memory size:  15800\n",
      "Episode:  79 - Score:  -128.42 - Average score:  -166.144 - Memory size:  16000\n",
      "Episode:  80 - Score:  -125.399 - Average score:  -166.396 - Memory size:  16200\n",
      "Episode:  81 - Score:  -286.222 - Average score:  -153.848 - Memory size:  16400\n",
      "Episode:  82 - Score:  -242.047 - Average score:  -153.474 - Memory size:  16600\n",
      "Episode:  83 - Score:  -125.83 - Average score:  -153.375 - Memory size:  16800\n",
      "Episode:  84 - Score:  -125.427 - Average score:  -153.346 - Memory size:  17000\n",
      "Episode:  85 - Score:  -239.388 - Average score:  -159.295 - Memory size:  17200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m next_state \u001b[38;5;241m=\u001b[39m next_state\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, state_size)\n\u001b[1;32m     21\u001b[0m agent\u001b[38;5;241m.\u001b[39mstore_data(state, action, reward, next_state, done)\n\u001b[0;32m---> 22\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     24\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[0;32m~/Documents/GitHub/reinforcement-learning/module/td3.py:206\u001b[0m, in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape3:\n\u001b[1;32m    205\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_eval(states)\n\u001b[0;32m--> 206\u001b[0m     Q_values_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_eval_1([states, outputs])\n\u001b[1;32m    207\u001b[0m     actor_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_sum(Q_values_1)\n\u001b[1;32m    209\u001b[0m grads3 \u001b[38;5;241m=\u001b[39m tape3\u001b[38;5;241m.\u001b[39mgradient(actor_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_eval\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_env/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py:1066\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1060\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1061\u001b[0m       composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1062\u001b[0m           output_gradients))\n\u001b[1;32m   1063\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1064\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1066\u001b[0m flat_grad \u001b[38;5;241m=\u001b[39m \u001b[43mimperative_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimperative_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_sources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent:\n\u001b[1;32m   1075\u001b[0m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_watched_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_env/lib/python3.9/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_TapeGradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_env/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py:142\u001b[0m, in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    139\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m num_inputs\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# This does not work with v1 TensorArrays.\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecuting_eagerly_outside_functions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m control_flow_util\u001b[38;5;241m.\u001b[39mEnableControlFlowV2(ops\u001b[38;5;241m.\u001b[39mget_default_graph()):\n\u001b[1;32m    144\u001b[0m   gradient_name_scope \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgradient_tape/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m forward_pass_name_scope:\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_env/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:4756\u001b[0m, in \u001b[0;36mexecuting_eagerly_outside_functions\u001b[0;34m()\u001b[0m\n\u001b[1;32m   4731\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(v1\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexecuting_eagerly_outside_functions\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   4732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecuting_eagerly_outside_functions\u001b[39m():\n\u001b[1;32m   4733\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns True if executing eagerly, even if inside a graph function.\u001b[39;00m\n\u001b[1;32m   4734\u001b[0m \n\u001b[1;32m   4735\u001b[0m \u001b[38;5;124;03m  This function will check the outermost context for the program and see if\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4754\u001b[0m \u001b[38;5;124;03m    boolean, whether the outermost context is in eager mode.\u001b[39;00m\n\u001b[1;32m   4755\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4756\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecuting_eagerly\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   4757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   4758\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_env/lib/python3.9/site-packages/tensorflow/python/eager/context.py:2351\u001b[0m, in \u001b[0;36mexecuting_eagerly\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m default_execution_mode \u001b[38;5;241m==\u001b[39m EAGER_MODE\n\u001b[0;32m-> 2351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecuting_eagerly\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_env/lib/python3.9/site-packages/tensorflow/python/eager/context.py:1021\u001b[0m, in \u001b[0;36mContext.executing_eagerly\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecuting_eagerly\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns True if current thread has eager executing enabled.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1021\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_thread_local_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_eager\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training\n",
    "env = gym.make('Pendulum-v1')\n",
    "agent = td3.TD3Agent(state_size, action_size, batch_size=32,\n",
    "                training_start=200, update_period=1,\n",
    "                lr_critic=0.001, lr_actor=0.001,\n",
    "                min_action=-1, max_action=1)\n",
    "score_history = []\n",
    "avg_score_history = []\n",
    "n_episodes = 20000\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    done = False\n",
    "    truncated = False\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state[0], [1, state_size])\n",
    "    while not truncated and not done:\n",
    "        action = agent.act(state, use_noise=True, noise_label='Gaussian')\n",
    "        next_state, reward, done, truncated, _ = env.step(2 * action)\n",
    "        next_state = next_state.reshape(1, state_size)\n",
    "        agent.store_data(state, action, reward, next_state, done)\n",
    "        agent.learn()\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        \n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-40:])\n",
    "    avg_score_history.append(avg_score)\n",
    "    print('Episode: ', i, '- Score: ', round(score, 3), '- Average score: ', round(avg_score, 3), '- Memory size: ', len(agent.memory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch = random.sample(agent.memory, min(len(agent.memory), agent.batch_size))\n",
    "states, actions, rewards, next_states, dones = [tf.convert_to_tensor(x, dtype=tf.float32) for x in zip(*minibatch)]\n",
    "\n",
    "states = tf.squeeze(states)\n",
    "dones = tf.reshape(dones, shape=(-1, 1))\n",
    "rewards = tf.reshape(rewards, shape=(-1, 1))\n",
    "next_states = tf.squeeze(next_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions for the next states\n",
    "noise = np.random.normal(0, agent.noise_std, (agent.batch_size, agent.action_size))\n",
    "noise = np.clip(noise, -agent.noise_boundary, agent.noise_boundary)\n",
    "actions_next_states = tf.clip_by_value(agent.actor_target(next_states) + noise, agent.min_action, agent.max_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_value_next_states_1 = agent.critic_target_1([next_states, actions_next_states])\n",
    "Q_value_next_states_2 = agent.critic_target_2([next_states, actions_next_states])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = rewards + agent.gamma * (1 - dones) * tf.math.minimum(Q_value_next_states_1, Q_value_next_states_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape1:\n",
    "    Q_value_current_states_1 = agent.critic_eval_1([states, actions])\n",
    "    critic_loss_1 = tf.reduce_mean(tf.square(y - Q_value_current_states_1))\n",
    "\n",
    "grads1 = tape1.gradient(critic_loss_1, agent.critic_eval_1.trainable_variables)\n",
    "agent.opt_critic.apply_gradients(zip(grads1, agent.critic_eval_1.trainable_variables))\n",
    "\n",
    "with tf.GradientTape() as tape2:\n",
    "    Q_value_current_states_2 = agent.critic_eval_2([states, actions])\n",
    "    critic_loss_2 = tf.reduce_mean(tf.square(y - Q_value_current_states_2))\n",
    "\n",
    "grads2 = tape2.gradient(critic_loss_2, agent.critic_eval_2.trainable_variables)\n",
    "agent.opt_critic.apply_gradients(zip(grads2, agent.critic_eval_2.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape3:\n",
    "    out_puts = agent.actor_eval(states)\n",
    "    Q_values_1 = agent.critic_eval_1([states, out_puts])\n",
    "    actor_loss = tf.reduce_sum(Q_values_1)\n",
    "\n",
    "grads3 = tape3.gradient(actor_loss, agent.actor_eval.trainable_variables)\n",
    "agent.opt_actor.apply_gradients(zip(grads3, agent.actor_eval.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-235.24802729157517\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v1', render_mode='human')\n",
    "state = env.reset()\n",
    "state = np.reshape(state[0], [1, state_size])\n",
    "done = False\n",
    "truncated = False\n",
    "score = 0\n",
    "while not done and not truncated:\n",
    "    action = agent.act(state, use_noise=False)\n",
    "    next_state, reward, done, truncated, _ = env.step(2 * action)\n",
    "    next_state = next_state.reshape(1, state_size)\n",
    "    state = next_state\n",
    "    score += reward\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dqn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
